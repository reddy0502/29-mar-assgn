{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11da31d6-71fc-4ec8-892b-917092eba50c",
   "metadata": {},
   "source": [
    "1ans:\n",
    "\n",
    "Lasso Regression, also known as L1 regularization, is a type of linear regression that adds a penalty term to the ordinary least squares objective function. This penalty term is the sum of the absolute values of the regression coefficients multiplied by a tuning parameter, which is typically denoted as lambda or alpha.\n",
    "\n",
    "The objective of Lasso Regression is to minimize the sum of the squared residuals plus the penalty term, which encourages the coefficients of less important variables to be set to zero, effectively performing variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347bdbf-6e32-4e49-a733-6ac8f946ac6d",
   "metadata": {},
   "source": [
    "2ans:\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to perform automatic feature selection by shrinking the coefficients of less important variables towards zero. This is achieved by adding an L1 regularization penalty term to the objective function, which forces some of the coefficients to become exactly zero for a large enough value of the penalty parameter (lambda or alpha)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192c2e7-2cd8-4634-a94f-889e9244e1f9",
   "metadata": {},
   "source": [
    "3ans:\n",
    "\n",
    "The interpretation of the coefficients of a Lasso Regression model is similar to that of other linear regression models. However, due to the penalty term in the objective function, the coefficients in a Lasso Regression model may be shrunk towards zero or set to exactly zero, depending on the value of the tuning parameter.\n",
    "\n",
    "The coefficients of a Lasso Regression model can be interpreted as the strength and direction of the association between the predictor variables and the response variable, after accounting for the effects of other predictor variables, and can also be used for feature selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66aa9e-0baa-4fbc-9123-c2da0d1ebcf9",
   "metadata": {},
   "source": [
    "4ans:\n",
    "\n",
    "The intercept term in Lasso Regression represents the expected value of the response variable when all predictor variables are zero. In some cases, it may be appropriate to force the intercept to be zero, particularly if the data has been centered prior to modeling. However, in most cases, the intercept is left unconstrained and estimated from the data.\n",
    "\n",
    "In summary, the tuning parameters in Lasso Regression control the trade-off between model complexity and goodness of fit, and must be chosen carefully to balance bias and variance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee8f7f-0fb1-4efe-84ac-d5e32dd4b6a6",
   "metadata": {},
   "source": [
    "5ans:\n",
    "\n",
    "Lasso Regression is a linear regression technique that assumes a linear relationship between the response variable and the predictor variables. Therefore, it is not suitable for non-linear regression problems where the relationship between the response variable and the predictor variables is non-linear.\n",
    "\n",
    " Lasso Regression can be extended to non-linear regression problems by transforming the predictor variables using non-linear functions, such as polynomials or splines. This is known as polynomial or spline regression, respectively. In this case, the Lasso penalty term is applied to the coefficients of the transformed variables, allowing for automatic variable selection and estimation of the non-linear relationship between the response variable and the transformed predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a619193-3e18-4e98-ba97-392e5268b95d",
   "metadata": {},
   "source": [
    "6ans:\n",
    "\n",
    "The main differences between Ridge Regression and Lasso Regression are:\n",
    "\n",
    "Penalty term: Ridge Regression adds a penalty term proportional to the square of the magnitude of the coefficients (L2 penalty), while Lasso Regression adds a penalty term proportional to the absolute value of the coefficients (L1 penalty).\n",
    "\n",
    "Shrinkage: Ridge Regression shrinks the coefficients towards zero without setting them exactly to zero, while Lasso Regression can set some coefficients exactly to zero, resulting in sparse models with fewer predictor variables.\n",
    "\n",
    "Variable selection: Ridge Regression includes all predictor variables in the model, although some of the coefficients may be small. In contrast, Lasso Regression can exclude some predictor variables from the model by setting their coefficients to zero, resulting in automatic variable selection and simpler models.\n",
    "\n",
    "Bias-variance trade-off: Ridge Regression balances the bias-variance trade-off by reducing the variance of the model at the cost of slightly increasing its bias. Lasso Regression can lead to more biased models but with lower variance due to the sparse nature of the resulting models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1dadd6-c8fe-4c75-b277-fe8dea42fba8",
   "metadata": {},
   "source": [
    "7ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
